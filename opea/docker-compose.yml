version: "3.8"

services:
  ollama-server:
    image: ollama/ollama
    container_name: ollama-server
    # Host port 8008 will map to container port 11434 by default
    ports:
      - "${LLM_ENDPOINT_PORT:-8008}:11434"
    environment:
      # Proxy settings (optional)
      - no_proxy=${no_proxy}
      - http_proxy=${http_proxy}
      - https_proxy=${https_proxy}
      # Specify model ID to be used by Ollama
      - LLM_MODEL_ID=${LLM_MODEL_ID:-"llama3.2:1b"}
    volumes:
      # Store downloaded models in the named volume so they persist
      - models:/root/.cache/ollama

  my-app:
    container_name: my-app
    build: .
    ports:
      - "8000:8000"
    depends_on:
      - ollama-server
    environment:
      # The hostname of the Ollama container for internal Docker network
      - OLLAMA_SERVER_HOST=ollama-server
      - OLLAMA_SERVER_PORT=11434
    command: uvicorn app:app --host 0.0.0.0 --port 8000

networks:
  default:
    driver: bridge

volumes:
  models:
